{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oZw3lh8d_e8N",
        "YDEHfMGkGV8J",
        "Pf69K7s6PLiz",
        "nOS59AIPPlfj",
        "1YTNZjsfdh8W",
        "9eikXM0IP0XN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b3fa098265cb41a491a00db86483b1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_761dfdab66bf4abf8b4b5be7041edaa0",
              "IPY_MODEL_27a3c8ce545f477d8b3cb2d4358b72aa",
              "IPY_MODEL_f007e15b78cf449c8d93945b0a5378d7"
            ],
            "layout": "IPY_MODEL_4e9cd596a218497a94940db9ee24471d"
          }
        },
        "761dfdab66bf4abf8b4b5be7041edaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78b3403352eb4fc68274d87602e3fa38",
            "placeholder": "​",
            "style": "IPY_MODEL_0dff3ccc15ab40399ea1b3749c9e7f01",
            "value": "100%"
          }
        },
        "27a3c8ce545f477d8b3cb2d4358b72aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_709f170efcf74bd484939ba2ff39dfb1",
            "max": 2005,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a368c89b81e3463a822042826c25b5c9",
            "value": 2005
          }
        },
        "f007e15b78cf449c8d93945b0a5378d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8da11b77ccb2437cb1ceaa82a30986ac",
            "placeholder": "​",
            "style": "IPY_MODEL_530861e7ab3146cd85d239b36fe8c228",
            "value": " 2005/2005 [01:04&lt;00:00, 40.79it/s]"
          }
        },
        "4e9cd596a218497a94940db9ee24471d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78b3403352eb4fc68274d87602e3fa38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dff3ccc15ab40399ea1b3749c9e7f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "709f170efcf74bd484939ba2ff39dfb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a368c89b81e3463a822042826c25b5c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8da11b77ccb2437cb1ceaa82a30986ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "530861e7ab3146cd85d239b36fe8c228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization"
      ],
      "metadata": {
        "id": "oZw3lh8d_e8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "data_fn = \"/home/jason/Desktop/projects/masters/ml/group_project/Dataset 100/Data/\"\n",
        "onlyfiles = [f for f in listdir(data_fn) if isfile(join(data_fn, f))]"
      ],
      "metadata": {
        "id": "kR3KBPXsSAvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.datasets.folder import pil_loader\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "sum = torch.Tensor([0., 0., 0.]).to(device)\n",
        "sq_sum = torch.Tensor([0., 0., 0.]).to(device)\n",
        "count = 0\n",
        "for fn in tqdm(onlyfiles):\n",
        "    image = transform(pil_loader(data_fn + fn)).to(device)\n",
        "    _, n, m = image.shape\n",
        "    sum += image.sum((1, 2))\n",
        "    sq_sum += (image**2).sum((1, 2))\n",
        "    count += n * m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b3fa098265cb41a491a00db86483b1d5",
            "761dfdab66bf4abf8b4b5be7041edaa0",
            "27a3c8ce545f477d8b3cb2d4358b72aa",
            "f007e15b78cf449c8d93945b0a5378d7",
            "4e9cd596a218497a94940db9ee24471d",
            "78b3403352eb4fc68274d87602e3fa38",
            "0dff3ccc15ab40399ea1b3749c9e7f01",
            "709f170efcf74bd484939ba2ff39dfb1",
            "a368c89b81e3463a822042826c25b5c9",
            "8da11b77ccb2437cb1ceaa82a30986ac",
            "530861e7ab3146cd85d239b36fe8c228"
          ]
        },
        "id": "7hy9Fp9ASNej",
        "outputId": "66b4afe7-7ff8-4eb8-a565-4117d346ccb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2005 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3fa098265cb41a491a00db86483b1d5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "means = sum/count\n",
        "stds = torch.sqrt(sq_sum/count - means**2)"
      ],
      "metadata": {
        "id": "_jyDjeQ1bNRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "means, stds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P80cTK_PbRX2",
        "outputId": "dc813ba0-9858-429a-af5c-e73170ade6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.5138, 0.4915, 0.4315], device='cuda:0'),\n",
              " tensor([0.2675, 0.2572, 0.2626], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Δεδομένα στο Drive."
      ],
      "metadata": {
        "id": "YDEHfMGkGV8J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYHloEqmDJKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1a0208-6fb7-469e-d3d0-30e385714838"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "\n",
        "* transforms\n",
        "* τροπος που φορτωνονται τα δεδομενα\n",
        "* augmentation"
      ],
      "metadata": {
        "id": "Pf69K7s6PLiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets.folder import pil_loader\n",
        "\n",
        "class Artists(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_path, image_ids_fn, images_dir):\n",
        "        self.base_path = base_path\n",
        "        self.image_ids_fn = image_ids_fn\n",
        "        self.images_dir = images_dir\n",
        "        with open(base_path + image_ids_fn, 'r') as fp:\n",
        "            rows = list(fp)\n",
        "            self.fnames = [s.strip().split(',')[0] for s in rows[1:]]\n",
        "            self.img_class_ids = [int(s.strip().split(',')[1]) for s in rows[1:]]\n",
        "            self.img_ids = list(range(len(self.fnames)))\n",
        "        self.transform = transforms.Compose((\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5138, 0.4915, 0.4315), (0.2675, 0.2572, 0.2626)),\n",
        "        ))\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_fname = self.fnames[index]\n",
        "        image = pil_loader(self.base_path + self.images_dir + img_fname)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, self.img_class_ids[index], self.img_ids[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n"
      ],
      "metadata": {
        "id": "i54tTXN2-rv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Μοντέλο\n",
        "\n",
        "* αρχιτεκτονικη\n",
        "* βαθος cnn\n",
        "* βαθος fc\n",
        "* regularization\n",
        "* fine-tuning/freeze\n"
      ],
      "metadata": {
        "id": "nOS59AIPPlfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class ResNet34Small(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNet34Small, self).__init__()\n",
        "        original_model = models.resnet34(pretrained=True)\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-3])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Qdu2fqtzHmFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "1YTNZjsfdh8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import ToTensor\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "def run(net, device, loader, optimizer, split='val', epoch=0, train=False,\n",
        "        dry_run=False):\n",
        "    if train:\n",
        "        net.train()\n",
        "        torch.set_grad_enabled(True)\n",
        "    else:\n",
        "        net.eval()\n",
        "        torch.set_grad_enabled(False)\n",
        "    \n",
        "    loader = tqdm(\n",
        "        loader,\n",
        "        ncols=0,\n",
        "        desc='{1} E{0:02d}'.format(epoch, 'train' if train else 'val')\n",
        "    )\n",
        "    \n",
        "    running_loss = 0\n",
        "    preds_all = []\n",
        "    labels_all = []\n",
        "    for (imgs, img_class_ids) in loader:\n",
        "        imgs, img_class_ids = (\n",
        "            imgs.to(device), img_class_ids.to(device).long()\n",
        "            )\n",
        "        \n",
        "        output = net(imgs)\n",
        "        _, preds = torch.max(output, 1)\n",
        "        loss = F.cross_entropy(output, img_class_ids)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += float(loss)\n",
        "        labels_all.extend(img_class_ids.cpu().numpy())\n",
        "        preds_all.extend(preds.cpu().numpy())\n",
        "        \n",
        "        if dry_run:\n",
        "            break\n",
        "\n",
        "    bal_acc = metrics.balanced_accuracy_score(labels_all, preds_all)\n",
        "\n",
        "    print('Epoch: {}.. '.format(epoch),\n",
        "        '{} Loss: {:.3f}.. '.format(split, running_loss / len(loader)),\n",
        "        '{} Accuracy: {:.3f}.. '.format(split, bal_acc),\n",
        "        )\n",
        "    \n",
        "    return running_loss / len(loader)\n",
        "\n",
        "\n",
        "def train(base_path, net_type, train_ids_fname, val_ids_fname, train_pkl, val_pkl,\n",
        "          images_dir, model_fname, num_classes=20, batch_size=16, lr=1e-2,\n",
        "          epochs=10, device='cpu', num_workers=6, dry_run=False):\n",
        "\n",
        "    with open(base_path + train_pkl,\"rb\") as f:\n",
        "      train_dataset = pickle.load(f)\n",
        "    with open(base_path + val_pkl,\"rb\") as f:\n",
        "      val_dataset = pickle.load(f)\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    net = {\n",
        "        'resnet': ResNet34Small\n",
        "    }[net_type](num_classes)\n",
        "    net.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,\n",
        "        net.parameters()), lr=lr)\n",
        "    \n",
        "    if device == 'cuda':\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    cur_best_val_loss = np.inf\n",
        "    for epoch in range(epochs):\n",
        "        _ = run(net, device, train_loader, optimizer, split='train',\n",
        "                    epoch=epoch, train=True, dry_run=dry_run)\n",
        "        val_loss = run(net, device, val_loader, optimizer, split='val',\n",
        "                    epoch=epoch, train=False, dry_run=dry_run)\n",
        "\n",
        "        if cur_best_val_loss > val_loss:\n",
        "            if epoch > 0:\n",
        "                # remove previous best model\n",
        "                os.remove(model_fname)\n",
        "            torch.save(net.state_dict(), model_fname)\n",
        "            cur_best_val_loss = val_loss\n",
        "        \n",
        "        if dry_run:\n",
        "            break\n",
        "\n",
        "    # load best model for final evaluation\n",
        "    net = {\n",
        "        'resnet': ResNet34Small\n",
        "    }[net_type](num_classes)\n",
        "    net.to(device)\n",
        "    # load best model for final evaluation\n",
        "    net.load_state_dict(torch.load(model_fname))\n",
        "    print(\"\\nModel loaded from checkpoint for final evaluation\\n\")\n",
        "    run(net, device, val_loader, optimizer, split='val_best', epoch=0,\n",
        "        train=False)"
      ],
      "metadata": {
        "id": "Dwa1glfBd1Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle \n",
        "#base_path = \"/home/jason/Desktop/projects/masters/ml/group_project/Dataset 100/\"\n",
        "base_path = \"/content/gdrive/My Drive/art_recognition/Datasets/Dataset 100/\"\n",
        "train_ids_fname = \"train_100.csv\"\n",
        "val_ids_fname = \"val_100.csv\"\n",
        "train_pkl = \"train_100.pkl\"\n",
        "val_pkl = \"val_100.pkl\"\n",
        "images_dir = \"Data/\"\n",
        "#model_fname = \"/home/jason/Desktop/projects/masters/ml/group_project/Dataset 100/resnet.pt\"\n",
        "model_fname = \"/content/gdrive/My Drive/art_recognition/Datasets/Dataset 100/resnet.pt\"\n",
        "\n",
        "net_type = \"resnet\"\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "net_type = \"resnet\"\n",
        "train(base_path, net_type, train_ids_fname, val_ids_fname, train_pkl, val_pkl,\n",
        "      images_dir, model_fname, device=device, dry_run=True)"
      ],
      "metadata": {
        "id": "e7OMAPtUI-sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature extraction από CNN"
      ],
      "metadata": {
        "id": "9eikXM0IP0XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/gdrive/My Drive/art_recognition/Datasets/Dataset 100/\"\n",
        "train_pkl = \"train_100.pkl\"\n",
        "val_pkl = \"val_100.pkl\"\n",
        "test_pkl = \"test_100.pkl\"\n",
        "net = ResNet34Small(20)\n",
        "checkpoint = torch.load(\"/content/gdrive/My Drive/art_recognition/Datasets/Dataset 100/resnet.pt\")\n",
        "net.load_state_dict(checkpoint)\n",
        "with open(base_path + train_pkl,\"rb\") as f:\n",
        "  train_dataset = pickle.load(f)\n",
        "with open(base_path + test_pkl,\"rb\") as f:\n",
        "  test_dataset = pickle.load(f)\n",
        "with open(base_path + val_pkl,\"rb\") as f:\n",
        "  val_dataset = pickle.load(f)\n",
        "\n",
        "painting_features_train = [(net.features(x.unsqueeze(0)).flatten().numpy(), y) for x, y in train_dataset]\n",
        "painting_features_val = [(net.features(x.unsqueeze(0).flatten()).numpy(),y) for x, y in val_dataset]\n",
        "painting_features_test = [(net.features(x.unsqueeze(0).flatten()).numpy(),y) for x, y in test_dataset]\n",
        "paintings = {\"train\":painting_features_train, \"test\":painting_features_test, \"val\":painting_features_val} "
      ],
      "metadata": {
        "id": "MmczXhx5EilL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = open('painting_features.pkl', 'wb')\n",
        "pickle.dump(datasets, output)\n",
        "output.close()\n",
        "!cp painting_features.pkl '/content/gdrive/My Drive/art_recognition/Datasets/Dataset 100/'"
      ],
      "metadata": {
        "id": "dPJqKX1HD4s-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}